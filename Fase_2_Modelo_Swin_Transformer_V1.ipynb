{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly26NqNza4cH"
      },
      "source": [
        "# **0. Configuraciones**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXMIp-CcEIpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66035262-783b-459b-f288-1038cff7691e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wnpy440qau3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb32ecf6-e4f0-4e4e-f823-cbd35c5e09e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun  3 14:46:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#@title **A. INFORMACIÓN DE GPU**\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gp-4FEHrbYL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbb7e05-faea-467a-eff7-9d86a51db01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.17.3)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 75.0 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 93.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.11.0+cu113)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.46.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 torchmetrics-0.9.1 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.4.0)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rwightman/pytorch-image-models.git\n",
            "  Cloning https://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-kaid5516\n",
            "  Running command git clone -q https://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-kaid5516\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.6.2.dev0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.6.2.dev0) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.6.2.dev0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.6.2.dev0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.6.2.dev0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.6.2.dev0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.6.2.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.6.2.dev0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.6.2.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm==0.6.2.dev0) (2.10)\n",
            "Building wheels for collected packages: timm\n",
            "  Building wheel for timm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timm: filename=timm-0.6.2.dev0-py3-none-any.whl size=499021 sha256=240212fd74ecdff82caf35012f4456500b61fd4cd9f327df2a792d06ab5f9d33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4fzu_mhz/wheels/a0/ec/5f/289118b747739bb1e02e36cf3d7e759721e881c183653719dc\n",
            "Successfully built timm\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.6.2.dev0\n"
          ]
        }
      ],
      "source": [
        "#@title **B. INSTALANDO DEPENDENCIAS**\n",
        "#@markdown  **B.1 Instalando Pytorch Lighthing**\n",
        "!pip install pytorch-lightning\n",
        "#@markdown  **B.2 Instalando Mediapipe**\n",
        "!pip install mediapipe\n",
        "#@markdown **B.3 Instalando TIMM from github**\n",
        "!pip install git+https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8I_iwm-Tcbni"
      },
      "outputs": [],
      "source": [
        "#@title **C. IMPORTANDO LIBRERIAS**\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "import torchvision.models as models\n",
        "#import torchmetrics\n",
        "#from pytorch_lightning.metrics.functional import accuracy\n",
        "from torchmetrics.classification import Accuracy\n",
        "from torchvision import datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import sys, os\n",
        "from glob import glob\n",
        "import imageio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "import shutil\n",
        "from tqdm.notebook  import tqdm\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from sklearn import metrics as sk_metrics\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timm.list_models()"
      ],
      "metadata": {
        "id": "0Wu_vwxJcZVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbm-AzF2EhTm"
      },
      "source": [
        "# **1. Carga del Dataset**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi1obn8jQhR9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **funciones** { run: \"auto\" }\n",
        "import shutil\n",
        "from tqdm.notebook  import tqdm\n",
        "import random\n",
        "import os\n",
        "import zipfile\n",
        "random.seed(10)\n",
        "\n",
        "#root_zip_dataset = '/content/gdrive/MyDrive/Colab Notebooks/dataset/RWF2000Processed/frames-150.7z'\n",
        "#!cp \"{root_zip_dataset}\" .\n",
        "#!p7zip -d frames-150.7z\n",
        "\n",
        "def copy_data(data_path, data_type, n_samples, opt=0, all=0):\n",
        "  TRAIN_PATH = os.path.join(data_path, data_type)\n",
        "  list_dire_clases= [f.path for f in os.scandir(TRAIN_PATH) if f.is_dir()]\n",
        "  list_name_clases = [os.path.basename(x) for x in list_dire_clases]\n",
        "\n",
        "  for class_path, class_name in zip(list_dire_clases, list_name_clases):\n",
        "    if(opt == 1 and class_name==\"Contempt\"):\n",
        "      continue\n",
        "\n",
        "    files_annot = glob(os.path.join(class_path, \"*.jpg\"))\n",
        "\n",
        "    if all:\n",
        "      samples = files_annot\n",
        "    else:\n",
        "      samples = random.sample(files_annot, n_samples)\n",
        "    print(f\"Copiando de {TRAIN_PATH}\")\n",
        "    print(f\"Clase {class_name} tiene {len(samples)} registros seleccionados\")\n",
        "    #continue\n",
        "    data_new_dir = os.path.join(\"/content/data\", data_type, class_name)\n",
        "    if not os.path.exists(data_new_dir):\n",
        "      os.makedirs(data_new_dir)\n",
        "    \n",
        "    print(\"copiando, por favor espere!!!...\")\n",
        "    pbar = tqdm(total=len(samples))\n",
        "    for sample in samples:\n",
        "      pbar.update(1)\n",
        "      new_dir = os.path.join(data_new_dir, os.path.basename(sample))\n",
        "      shutil.copy(sample, new_dir)\n",
        "    pbar.close()\n",
        "\n",
        "def copy_data(root_data, dataset_name, clases = 7):\n",
        "  root_path = os.path.join(root_data,dataset_name)\n",
        "  save_path_class =\"data\"\n",
        "  if not os.path.exists(save_path_class):\n",
        "      os.makedirs(save_path_class)\n",
        "  \n",
        "  with zipfile.ZipFile(root_path, 'r') as zip_ref:\n",
        "    if clases == 7:\n",
        "      listOfFileNames = zip_ref.namelist()\n",
        "      for file_n in zip_ref.namelist():\n",
        "        if file_n.startswith('train/Contempt/') or file_n.startswith('val/Contempt/'):\n",
        "          continue\n",
        "        else:\n",
        "          zip_ref.extract(file_n, save_path_class)\n",
        "    else:\n",
        "      zip_ref.extractall(save_path_class)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "root_data = \"/content/drive/MyDrive/Datasets/dataset UCSP\"\n",
        "data_name = \"Affect_net_local_seg_alig.zip\"\n",
        "copy_data(root_data, data_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRiDoQdCuvOo",
        "outputId": "4d286a2d-0f0b-478e-f012-f27f57430f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 29.8 s, sys: 9.92 s, total: 39.7 s\n",
            "Wall time: 42.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GGLTyWOG6cO"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn1XY9fZFeom",
        "outputId": "7a808e4b-3243-4c66-92a3-7d16e3a8210c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase Contempt tiene 22500 registros\n",
            "Clase Surprise tiene 28180 registros\n",
            "Clase Disgust tiene 22818 registros\n",
            "Clase Sadness tiene 23000 registros\n",
            "Clase Neutral tiene 23000 registros\n",
            "Clase Happiness tiene 23000 registros\n",
            "Clase Anger tiene 23000 registros\n",
            "Clase Fear tiene 25512 registros\n"
          ]
        }
      ],
      "source": [
        "TRAIN_PATH = os.path.join(\"/content/data/train\")\n",
        "list_dire_clases= [f.path for f in os.scandir(TRAIN_PATH) if f.is_dir()]\n",
        "list_name_clases = [os.path.basename(x) for x in list_dire_clases]\n",
        "cantidades = []\n",
        "for class_path, class_name in zip(list_dire_clases, list_name_clases):\n",
        "  files_annot = glob(os.path.join(class_path, \"*.jpg\"))\n",
        "  print(f\"Clase {class_name} tiene {len(files_annot)} registros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRlygnefuLE"
      },
      "source": [
        "# **2. VISUALIZANDO EL DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wTUhM0lMf4GO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "8182fae4-5f88-4ca1-fc70-dab2fb9bdbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clase Surprise tiene 28180 registros\n",
            "Clase Disgust tiene 22818 registros\n",
            "Clase Sadness tiene 23000 registros\n",
            "Clase Neutral tiene 23000 registros\n",
            "Clase Happiness tiene 23000 registros\n",
            "Clase Anger tiene 23000 registros\n",
            "Clase Fear tiene 25512 registros\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6], <a list of 7 Text major ticklabel objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@title DATOS DE ENTRENAMIENTO\n",
        "# Datos de entrenamiento\n",
        "DATA_PATH=\"/content/data\"\n",
        "TRAIN_PATH = os.path.join(DATA_PATH, \"train\")\n",
        "list_dire_clases= [f.path for f in os.scandir(TRAIN_PATH) if f.is_dir()]\n",
        "list_name_clases = [os.path.basename(x) for x in list_dire_clases]\n",
        "cantidades = []\n",
        "limit = 4200\n",
        "print()\n",
        "for class_path, class_name in zip(list_dire_clases, list_name_clases):\n",
        "  files_annot = glob(os.path.join(class_path, \"*.jpg\"))\n",
        "  files_names = [os.path.basename(x) for x in files_annot]\n",
        "  print(f\"Clase {class_name} tiene {len(files_annot)} registros\")\n",
        "  tama = len(files_annot)\n",
        "  cantidades.append(len(files_annot))\n",
        "  #ajustar_affecnetdata(files_annot, files_names, class_name, limit)\n",
        "plt.bar(list_name_clases,cantidades)\n",
        "plt.xticks(rotation=45)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "viUH4UuojsyX",
        "outputId": "68cc6aa3-a4d1-40c1-f635-e363a2036bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase Surprise tiene 500 registros\n",
            "Clase Disgust tiene 500 registros\n",
            "Clase Sadness tiene 500 registros\n",
            "Clase Neutral tiene 500 registros\n",
            "Clase Happiness tiene 500 registros\n",
            "Clase Anger tiene 500 registros\n",
            "Clase Fear tiene 500 registros\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4, 5, 6], <a list of 7 Text major ticklabel objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@title DATOS DE VALIDACION\n",
        "# Datos de entrenamiento\n",
        "TRAIN_PATH = os.path.join(DATA_PATH, \"valid\")\n",
        "list_dire_clases= [f.path for f in os.scandir(TRAIN_PATH) if f.is_dir()]\n",
        "list_name_clases = [os.path.basename(x) for x in list_dire_clases]\n",
        "cantidades = []\n",
        "limit = 4200\n",
        "for class_path, class_name in zip(list_dire_clases, list_name_clases):\n",
        "  files_annot = glob(os.path.join(class_path, \"*\"))\n",
        "  files_names = [os.path.basename(x) for x in files_annot]\n",
        "  print(f\"Clase {class_name} tiene {len(files_annot)} registros\")\n",
        "  tama = len(files_annot)\n",
        "  cantidades.append(len(files_annot))\n",
        "  #ajustar_affecnetdata(files_annot, files_names, class_name, limit)\n",
        "plt.bar(list_name_clases,cantidades)\n",
        "plt.xticks(rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXW9ey_IIlcZ"
      },
      "source": [
        "#**3. Dataset definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2RUxiXUGbyK"
      },
      "outputs": [],
      "source": [
        "# https://discuss.pytorch.org/t/why-do-we-need-subsets-at-all/49391/7\n",
        "# adapted from ptrblck post\n",
        "class MyLazyDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.transform:\n",
        "            x = self.transform(self.dataset[index][0])\n",
        "        else:\n",
        "            x = self.dataset[index][0]      \n",
        "\n",
        "\n",
        "        #plt.imshow(x)    \n",
        "        y = self.dataset[index][1]\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKZ6qHaOELZ"
      },
      "source": [
        "### directory structure of the dataset\n",
        "```\n",
        "root\n",
        "|\n",
        "-- train\n",
        "|\n",
        "-- validation\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVgKmpTvIwHk"
      },
      "outputs": [],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dat_norm, data_dir: str, img_size = 224, batch_size = 200, align=True):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.align = align\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(size = self.img_size),\n",
        "            #transforms.CenterCrop(size = self.img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(dat_norm[0], dat_norm[1])\n",
        "        ])\n",
        "\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.Resize(size = self.img_size),\n",
        "            #transforms.RandomRotation(degrees=15),\n",
        "            #transforms.CenterCrop(size=self.img_size),\n",
        "            #transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(dat_norm[0], dat_norm[1])\n",
        "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = 4\n",
        "    \n",
        "    def setup(self, stage = None):\n",
        "        # train path\n",
        "        path_train = os.path.join(self.data_dir, 'train')\n",
        "        # test path\n",
        "        path_test =  os.path.join(self.data_dir, 'valid')\n",
        "        mydataset_train = datasets.ImageFolder(root = path_train)\n",
        "        mydataset_test  = datasets.ImageFolder(root = path_test)\n",
        "        self.path_test = path_test\n",
        "        self.categories =  mydataset_test.class_to_idx\n",
        "        traindataset = MyLazyDataset(mydataset_train, self.train_transform, rotate = self.align) # train\n",
        "        validdataset = MyLazyDataset(mydataset_train, self.train_transform, rotate = self.align) # validation\n",
        "        self.testdataset = MyLazyDataset(mydataset_test, self.transform, rotate = self.align) # test\n",
        "\n",
        "        # Create the index splits for training and validation  \n",
        "        # Train is divided into two subsets: train and validation\n",
        "        train_size = 0.9\n",
        "        num_train = len(mydataset_train)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(train_size * num_train))\n",
        "        np.random.shuffle(indices)\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        self.traindata = torch.utils.data.Subset(traindataset, indices=train_idx)\n",
        "        self.validdata = torch.utils.data.Subset(validdataset, indices=valid_idx)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            self.traindata, \n",
        "            batch_size=self.batch_size, \n",
        "            num_workers=self.num_workers, \n",
        "            drop_last=True)\n",
        "        return train_loader\n",
        "    def val_dataloader(self):\n",
        "        valid_loader = torch.utils.data.DataLoader(\n",
        "            self.validdata, \n",
        "            batch_size=self.batch_size, \n",
        "            num_workers=self.num_workers, \n",
        "            drop_last=True)\n",
        "        return valid_loader\n",
        "    def test_dataloader(self):\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            self.testdataset, \n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers, \n",
        "            drop_last=True)\n",
        "        return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkTransferLearning(pl.LightningModule):\n",
        "    def __init__(self, type_net, optimizer=\"Adam\", num_classes = 7, lr = 1e-3, pretrained = True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = lr\n",
        "        self.backbone = timm.create_model(type_net, pretrained = pretrained)\n",
        "        if pretrained:\n",
        "            # freeze  weights\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.num_in_feat = self.backbone.get_classifier().in_features\n",
        "\n",
        "        block = nn.Sequential(nn.Linear(self.num_in_feat, 1024), \n",
        "                              nn.Dropout(0.5), \n",
        "                              nn.Linear(1024, 512), \n",
        "                              #nn.Dropout(0.5), \n",
        "                              nn.Linear(512, self.num_classes))\n",
        "\n",
        "        name, module = list(self.backbone.named_children())[-1]\n",
        "        self.backbone._modules[name] = block        \n",
        "\n",
        "        # 3 Loss function\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 4 Metrics\n",
        "        self.train_acc = Accuracy()\n",
        "        self.valid_acc = Accuracy(compute_on_step=False)  \n",
        "        self.test_acc  = Accuracy(compute_on_step=False)       \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.backbone.forward(x)\n",
        "        #out = self.backbone(x)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        outputs = self(images)\n",
        "        \n",
        "        loss = self.loss(outputs, targets)\n",
        "\n",
        "        #preds = nn.functional.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        self.train_acc(preds, targets)\n",
        "\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', self.train_acc)\n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outs):\n",
        "        loss =self.train_acc.compute()        \n",
        "        self.log('avg_train_acc',loss)\n",
        "        print(f\"avg_train_acc: {loss}, \", end=\" \")\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss(outputs, targets)\n",
        "\n",
        "        #preds = nn.functional.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        self.valid_acc(preds, targets)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', self.valid_acc)\n",
        "        return loss\n",
        "    \n",
        "    def validation_epoch_end(self, val_step_outputs):\n",
        "        avg_val_acc = self.valid_acc.compute()\n",
        "        self.log('avg_val_acc',avg_val_acc)\n",
        "        print(f\"avg_val_acc: {avg_val_acc}\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.loss(outputs, targets)\n",
        "\n",
        "        #preds = nn.functional.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        #print(preds)\n",
        "        self.test_acc(preds, targets)\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', self.test_acc)\n",
        "        return loss\n",
        "    \n",
        "    def test_epoch_end(self, val_step_outputs):\n",
        "        self.log('avg_test_acc', self.test_acc.compute())\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      if self.optimizer == \"Adam\":\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "      else:\n",
        "        #torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "      return optimizer\n",
        "\n",
        "    def get_mean_std(self):\n",
        "      return self.backbone.default_cfg[\"mean\"], self.backbone.default_cfg[\"std\"]\n",
        "\n",
        "    def set_lr_optimizer(self, lr, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = lr\n",
        "     \n",
        "\n"
      ],
      "metadata": {
        "id": "1wxNkL9FvPtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "def train_process():\n",
        "  from pytorch_lightning import callbacks as pl_callbacks\n",
        "  from sklearn import metrics as sk_metrics\n",
        "  callback_model_checkpoint = pl_callbacks.ModelCheckpoint(\n",
        "      '{epoch}-{valid_loss:.3f}', \n",
        "      monitor='val_loss', \n",
        "      mode='min'\n",
        "  )\n",
        "\n",
        "  chk_dir = \"Codigos\"\n",
        "  img_size = 224\n",
        "  base_dir = \"/content/data\"#/content/drive/MyDrive/UNIVERSIDAD CATÓLICA PROYECTOS/EMOTION RECOGNITION/Datasets/Affect_net_local_gray\"\n",
        "  num_class = 7\n",
        "  batch_size=150 #32\n",
        "  \n",
        "  model = 'swin_large_patch4_window7_224_in22k'\n",
        "\n",
        "  optimizer = \"Adam\"\n",
        "  lr = 1e-6\n",
        "  modelo = NetworkTransferLearning(type_net = model, optimizer = optimizer, num_classes=num_class, lr=lr)\n",
        "  dm = DataModule(modelo.get_mean_std(),data_dir = base_dir, img_size = img_size, batch_size=batch_size, align = True)\n",
        "  trainer = Trainer(fast_dev_run=False, default_root_dir=chk_dir, gpus=1, progress_bar_refresh_rate=20, max_epochs=10, checkpoint_callback=callback_model_checkpoint)\n",
        "  n = len(list(modelo.parameters()))\n",
        "  for i, param in enumerate(modelo.parameters()):\n",
        "    if i>n*0.70:\n",
        "      param.requires_grad = True\n",
        "  trainer.fit(modelo, dm)\n",
        "  trainer.test(modelo, datamodule=dm)"
      ],
      "metadata": {
        "id": "oTjgSES_dUXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "IiGOpWEbUd1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_process()"
      ],
      "metadata": {
        "id": "332zDZwLCK3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg1p9ju9GdVw"
      },
      "source": [
        "# **ETAPA DE PRUEBAS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"Codigos/lightning_logs\""
      ],
      "metadata": {
        "id": "K4aOHCQoZJvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title func\n",
        "@torch.no_grad()\n",
        "def get_all_preds(model, loader):\n",
        "    #device = \"cpu\"\n",
        "\n",
        "    all_preds = torch.tensor([]).to(\"cuda:0\")\n",
        "    all_labels = torch.tensor([]).to(\"cuda:0\")\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        print(f\"Cargando batch {i+1}\")\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = model(x)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "        all_labels = torch.cat(\n",
        "            (all_labels, y)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds, all_labels"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uo-lj7qWbNPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bYZsPlUkE41F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ako5soMSPTB3"
      },
      "outputs": [],
      "source": [
        "model_t = 'swin_large_patch4_window7_224_in22k'\n",
        "modelo2 = NetworkTransferLearning.load_from_checkpoint(\"Codigos/lightning_logs/version_63/checkpoints/epoch=32-step=9702.ckpt\", type_net=model_t, num_classes = 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNZWlDnhRrI0"
      },
      "outputs": [],
      "source": [
        "#base_dir = DATA_PATH \"/content/drive/MyDrive/UNIVERSIDAD CATÓLICA PROYECTOS/EMOTION RECOGNITION/Datasets/DATA_PATH\"\n",
        "#test_path = \"/content/drive/MyDrive/UNIVERSIDAD CATÓLICA PROYECTOS/EMOTION RECOGNITION/Datasets/DATA_PATH\"\n",
        "img_size = 224\n",
        "dm = DataModule(modelo2.get_mean_std(), data_dir = \"/content/data\", img_size = img_size, batch_size=1)\n",
        "dm.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is5WBnIv0IB1"
      },
      "outputs": [],
      "source": [
        "y_true, y_pred = [],[]\n",
        "#modelo.freeze()\n",
        "#modelo.eval()\n",
        "#loader = dm.test_dataloader()\n",
        "#for i, (x, y) in enumerate(loader):\n",
        "#    print(f\"Cargando batch {i+1}\")\n",
        "#    #y_pred.extend([x.shape[0]])\n",
        "#    y_true.extend(list(y))\n",
        "#    #y_hat = modelo(x)#modelo.forward(x).argmax(axis=1).cpu().detach().numpy()\n",
        "#    pred_probs =  modelo(x)\n",
        "#    y_pred.extend(pred_probs.argmax(axis=-1).cpu().numpy())\n",
        "#    #y_true.append(y)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from resources.plotcm import plot_confusion_matrix\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo2 = modelo2.to(device)\n",
        "loader = dm.test_dataloader()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_preds, all_labels = get_all_preds(modelo2, loader)\n",
        "\n",
        "y_true = all_labels.cpu()\n",
        "y_pred =test_preds.argmax(dim=1).cpu()\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tot = np.sum(cm, axis=1)\n",
        "cm_porc = np.divide(cm, np.reshape(tot, (-1,1)))\n",
        "#print(cm_porc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNaoc8hLk45t"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy score on test data:\\t{sk_metrics.accuracy_score(y_true, y_pred)}')\n",
        "print(f'Macro F1 score on test data:\\t{sk_metrics.f1_score(y_true, y_pred, average=\"macro\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KszNOGpLE_S-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Matriz de confusión en porcentajes\n",
        "labels = list(dm.categories)\n",
        "# Calculate confusion matrix\n",
        "#confusion_matrix = sk_metrics.confusion_matrix(y_true, y_pred)\n",
        "df_confusion_matrix = pd.DataFrame(cm_porc, index=labels, columns=labels)\n",
        "\n",
        "# Show confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "sn.heatmap(df_confusion_matrix, annot=True, cbar=False, cmap='Oranges', linewidths=1, linecolor='black', fmt=\".2%\")\n",
        "plt.xlabel('Predicted labels', fontsize=15)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.ylabel('True labels', fontsize=15)\n",
        "plt.yticks(fontsize=12);\n",
        "#plt.savefig(os.path.join(RESULTS_PATH, \"confusion_matrix_porc.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zcblrjQZCzQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Matriz de confusión con cantidades\n",
        "labels = list(dm.categories)\n",
        "# Calculate confusion matrix\n",
        "#confusion_matrix = sk_metrics.confusion_matrix(y_true, y_pred)\n",
        "df_confusion_matrix = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "# Show confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "sn.heatmap(df_confusion_matrix, annot=True, cbar=False, cmap='Oranges', linewidths=1, linecolor='black', fmt='g')\n",
        "plt.xlabel('Predicted labels', fontsize=15)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.ylabel('True labels', fontsize=15)\n",
        "plt.yticks(fontsize=12);\n",
        "#plt.savefig(os.path.join(RESULTS_PATH, \"confusion_matrix.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjokK857VaWL"
      },
      "outputs": [],
      "source": [
        "report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
        "print(classification_report(y_true, y_pred, target_names=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probar **nuevamente**"
      ],
      "metadata": {
        "id": "OB3Tz9zmO5dB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_t = 'swin_large_patch4_window7_224_in22k'\n",
        "optimizer = \"Adam\"\n",
        "lr = 1e-6\n",
        "modelo2 = NetworkTransferLearning.load_from_checkpoint(\"/content/drive/MyDrive/Universidad UCSP/Proyecto Concytec/Codigos/lightning_logs/version_66/checkpoints/epoch=56-step=16758.ckpt\", type_net=model_t, optimizer = optimizer, num_classes=7, lr=lr)"
      ],
      "metadata": {
        "id": "smRWIYNLFgnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b88f1b-45e0-4652-9dc6-9a274ac0f7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth\" to /root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22k.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import callbacks as pl_callbacks\n",
        "from sklearn import metrics as sk_metrics\n",
        "callback_model_checkpoint = pl_callbacks.ModelCheckpoint(\n",
        "      '{epoch}-{valid_loss:.3f}', \n",
        "      monitor='val_loss', \n",
        "      mode='min'\n",
        "  )\n",
        "\n",
        "chk_dir = \"/content/drive/MyDrive/Universidad UCSP/Proyecto Concytec/Codigos\"\n",
        "img_size = 224\n",
        "base_dir = \"/content/data\"#/content/drive/MyDrive/UNIVERSIDAD CATÓLICA PROYECTOS/EMOTION RECOGNITION/Datasets/Affect_net_local_gray\"\n",
        "num_class = 7\n",
        "dm = DataModule(modelo2.get_mean_std(),data_dir = base_dir, img_size = img_size, batch_size=100, align = True)\n",
        "n = len(list(modelo2.parameters()))\n",
        "for i, param in enumerate(modelo2.parameters()):\n",
        "  if i>n*0.70:\n",
        "    param.requires_grad = True\n",
        "trainer = Trainer(fast_dev_run=False, default_root_dir=chk_dir, gpus=1, progress_bar_refresh_rate=20, max_epochs=40, checkpoint_callback=callback_model_checkpoint)\n",
        "trainer.fit(modelo2, dm)\n",
        "trainer.test(modelo2, datamodule=dm)"
      ],
      "metadata": {
        "id": "zKcvjWe4O9gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D1r4b7eNz34Z"
      ],
      "machine_shape": "hm",
      "name": "Fase 2. Modelo Swin Transformer - V1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}